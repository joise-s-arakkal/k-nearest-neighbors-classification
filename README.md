# ðŸš€ Task 6: K-Nearest Neighbors (KNN) Classification

This project implements the K-Nearest Neighbors (KNN) algorithm on the Iris dataset using scikit-learn. The workflow includes data normalization, experimenting with different values of **K**, evaluating the model with accuracy and a confusion matrix, and visualizing the decision boundaries.  

---

## ðŸ“‚ Dataset

- **Name:** Iris Dataset  
- **Source:** [Kaggle Link](https://www.kaggle.com/datasets/uciml/iris)  
- **Classes:** Setosa, Versicolor, Virginica  
- **Features:** Sepal Length, Sepal Width, Petal Length, Petal Width  

---

## ðŸ›  Tools & Libraries

- Python (Colab)  
- scikit-learn  
- matplotlib  
- seaborn  
- pandas  
- opendatasets  

---

## ðŸ“Œ Steps Implemented

âœ… Load data using **opendatasets** from Kaggle  
âœ… View and explore with **pandas**  
âœ… Normalize features using **StandardScaler**  
âœ… Use **KNeighborsClassifier** for classification  
âœ… Test different values of **K** (1, 3, 5, 7, 9)  
âœ… Evaluate with accuracy score and confusion matrix  
âœ… Visualize decision boundaries (first two features)  

---

## ðŸ“Š Results

For each K value, the accuracy and confusion matrix were printed to analyze the model performance. A decision boundary plot shows how KNN separates classes in the 2D feature space.

![Visualize decision boundaries](vis_decision_boundary.png)

> The decision boundary was plotted using the first two features (sepal length and sepal width) after normalization, with `K=5`.  
